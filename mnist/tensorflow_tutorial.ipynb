{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 37800\n",
      "number of test examples = 4200\n",
      "X_train shape: (37800, 28, 28, 1)\n",
      "Y_train shape: (37800, 10)\n",
      "X_test shape: (4200, 28, 28, 1)\n",
      "Y_test shape: (4200, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "X = (train.iloc[:,1:].values).astype('float32') / 255.0\n",
    "Y = train.iloc[:,0].values.astype('int32')\n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y = to_categorical(Y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"./test.csv\")\n",
    "X_score = (test.values).astype('float32')/255.0\n",
    "X_score = X_score.reshape(X_score.shape[0], 28, 28, 1)\n",
    "print(X_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# GRADED FUNCTION: random_mini_batches\n",
    "def random_mini_batches(X, Y, mini_batch_size = 100, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation]\n",
    "    shuffled_Y = Y[permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches*mini_batch_size:]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"A deep MNIST classifier using convolutional layers.\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "\"\"\"\n",
    "# Disable linter warnings to maintain consistency with tutorial.\n",
    "# pylint: disable=invalid-name\n",
    "# pylint: disable=g-bad-import-order\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def deepnn(x_image):\n",
    "  \"\"\"deepnn builds the graph for a deep net for classifying digits.\n",
    "  Args:\n",
    "    x: an input tensor with the dimensions (N_examples, 784), where 784 is the\n",
    "    number of pixels in a standard MNIST image.\n",
    "  Returns:\n",
    "    A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "    equal to the logits of classifying the digit into one of 10 classes (the\n",
    "    digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "    dropout.\n",
    "  \"\"\"\n",
    "  # Reshape to use within a convolutional neural net.\n",
    "  # Last dimension is for \"features\" - there is only one here, since images are\n",
    "  # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n",
    "#   with tf.name_scope('reshape'):\n",
    "#     x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "  # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "  with tf.name_scope('conv1'):\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "  # Pooling layer - downsamples by 2X.\n",
    "  with tf.name_scope('pool1'):\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "  # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "  with tf.name_scope('conv2'):\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "  # Second pooling layer.\n",
    "  with tf.name_scope('pool2'):\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n",
    "  # is down to 7x7x64 feature maps -- maps this to 1024 features.\n",
    "  with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "  # features.\n",
    "  with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "  # Map the 1024 features to 10 classes, one for each digit\n",
    "  with tf.name_scope('fc2'):\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "  return y_conv, keep_prob\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def run_network(num_epochs = 25, minibatch_size = 50):\n",
    "  # Create the model\n",
    "  x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "\n",
    "  # Define loss and optimizer\n",
    "  y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "  # Build the graph for the deep net\n",
    "  y_conv, keep_prob = deepnn(x)\n",
    "\n",
    "  with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,\n",
    "                                                            logits=y_conv)\n",
    "  cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "  with tf.name_scope('adam_optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "  with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "  accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "#   graph_location = tempfile.mkdtemp()\n",
    "#   print('Saving graph to: %s' % graph_location)\n",
    "#   train_writer = tf.summary.FileWriter(graph_location)\n",
    "#   train_writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "  seed = 3\n",
    "  m = X_train.shape[0]\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epochs):\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "        \n",
    "        j = 0\n",
    "        for minibatch in minibatches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            train_step.run(feed_dict={x: minibatch_X, y_:minibatch_Y, keep_prob: 0.5})\n",
    "        \n",
    "            j = j+1\n",
    "            if j % 100 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={x: minibatch_X, y_: minibatch_Y, keep_prob: 1.0})\n",
    "                print('step %d, batch %d, training accuracy %g' % (i, j, train_accuracy))\n",
    "            \n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: X_test, y_: Y_test, keep_prob: 1.0}))\n",
    "    \n",
    "    predict_op = tf.argmax(y_conv, 1)\n",
    "    predictions = []\n",
    "    for i in range(int(X_score.shape[0]/50)):\n",
    "        p = sess.run(predict_op, {x: X_score[i*50:(i+1)*50], keep_prob: 1.0})\n",
    "        predictions.extend(p)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, batch 100, training accuracy 0.88\n",
      "step 0, batch 200, training accuracy 0.9\n",
      "step 0, batch 300, training accuracy 0.94\n",
      "step 0, batch 400, training accuracy 0.96\n",
      "step 0, batch 500, training accuracy 0.96\n",
      "step 0, batch 600, training accuracy 0.96\n",
      "step 0, batch 700, training accuracy 0.98\n",
      "step 1, batch 100, training accuracy 0.98\n",
      "step 1, batch 200, training accuracy 0.96\n",
      "step 1, batch 300, training accuracy 0.96\n",
      "step 1, batch 400, training accuracy 1\n",
      "step 1, batch 500, training accuracy 0.98\n",
      "step 1, batch 600, training accuracy 0.96\n",
      "step 1, batch 700, training accuracy 1\n",
      "step 2, batch 100, training accuracy 0.98\n",
      "step 2, batch 200, training accuracy 0.98\n",
      "step 2, batch 300, training accuracy 0.98\n",
      "step 2, batch 400, training accuracy 0.98\n",
      "step 2, batch 500, training accuracy 1\n",
      "step 2, batch 600, training accuracy 1\n",
      "step 2, batch 700, training accuracy 0.98\n",
      "step 3, batch 100, training accuracy 0.96\n",
      "step 3, batch 200, training accuracy 1\n",
      "step 3, batch 300, training accuracy 0.98\n",
      "step 3, batch 400, training accuracy 0.98\n",
      "step 3, batch 500, training accuracy 1\n",
      "step 3, batch 600, training accuracy 1\n",
      "step 3, batch 700, training accuracy 0.98\n",
      "step 4, batch 100, training accuracy 1\n",
      "step 4, batch 200, training accuracy 0.98\n",
      "step 4, batch 300, training accuracy 0.98\n",
      "step 4, batch 400, training accuracy 0.98\n",
      "step 4, batch 500, training accuracy 0.98\n",
      "step 4, batch 600, training accuracy 1\n",
      "step 4, batch 700, training accuracy 1\n",
      "step 5, batch 100, training accuracy 1\n",
      "step 5, batch 200, training accuracy 1\n",
      "step 5, batch 300, training accuracy 1\n",
      "step 5, batch 400, training accuracy 1\n",
      "step 5, batch 500, training accuracy 1\n",
      "step 5, batch 600, training accuracy 1\n",
      "step 5, batch 700, training accuracy 1\n",
      "step 6, batch 100, training accuracy 0.96\n",
      "step 6, batch 200, training accuracy 1\n",
      "step 6, batch 300, training accuracy 0.98\n",
      "step 6, batch 400, training accuracy 1\n",
      "step 6, batch 500, training accuracy 0.98\n",
      "step 6, batch 600, training accuracy 1\n",
      "step 6, batch 700, training accuracy 1\n",
      "step 7, batch 100, training accuracy 1\n",
      "step 7, batch 200, training accuracy 1\n",
      "step 7, batch 300, training accuracy 1\n",
      "step 7, batch 400, training accuracy 0.96\n",
      "step 7, batch 500, training accuracy 0.98\n",
      "step 7, batch 600, training accuracy 1\n",
      "step 7, batch 700, training accuracy 0.98\n",
      "step 8, batch 100, training accuracy 1\n",
      "step 8, batch 200, training accuracy 1\n",
      "step 8, batch 300, training accuracy 1\n",
      "step 8, batch 400, training accuracy 1\n",
      "step 8, batch 500, training accuracy 1\n",
      "step 8, batch 600, training accuracy 1\n",
      "step 8, batch 700, training accuracy 0.98\n",
      "step 9, batch 100, training accuracy 0.98\n",
      "step 9, batch 200, training accuracy 1\n",
      "step 9, batch 300, training accuracy 1\n",
      "step 9, batch 400, training accuracy 1\n",
      "step 9, batch 500, training accuracy 0.98\n",
      "step 9, batch 600, training accuracy 0.98\n",
      "step 9, batch 700, training accuracy 1\n",
      "step 10, batch 100, training accuracy 1\n",
      "step 10, batch 200, training accuracy 1\n",
      "step 10, batch 300, training accuracy 0.98\n",
      "step 10, batch 400, training accuracy 1\n",
      "step 10, batch 500, training accuracy 1\n",
      "step 10, batch 600, training accuracy 1\n",
      "step 10, batch 700, training accuracy 1\n",
      "step 11, batch 100, training accuracy 1\n",
      "step 11, batch 200, training accuracy 1\n",
      "step 11, batch 300, training accuracy 1\n",
      "step 11, batch 400, training accuracy 1\n",
      "step 11, batch 500, training accuracy 1\n",
      "step 11, batch 600, training accuracy 1\n",
      "step 11, batch 700, training accuracy 1\n",
      "step 12, batch 100, training accuracy 1\n",
      "step 12, batch 200, training accuracy 1\n",
      "step 12, batch 300, training accuracy 1\n",
      "step 12, batch 400, training accuracy 1\n",
      "step 12, batch 500, training accuracy 1\n",
      "step 12, batch 600, training accuracy 0.96\n",
      "step 12, batch 700, training accuracy 1\n",
      "step 13, batch 100, training accuracy 1\n",
      "step 13, batch 200, training accuracy 0.98\n",
      "step 13, batch 300, training accuracy 0.98\n",
      "step 13, batch 400, training accuracy 1\n",
      "step 13, batch 500, training accuracy 1\n",
      "step 13, batch 600, training accuracy 1\n",
      "step 13, batch 700, training accuracy 1\n",
      "step 14, batch 100, training accuracy 1\n",
      "step 14, batch 200, training accuracy 1\n",
      "step 14, batch 300, training accuracy 1\n",
      "step 14, batch 400, training accuracy 1\n",
      "step 14, batch 500, training accuracy 1\n",
      "step 14, batch 600, training accuracy 1\n",
      "step 14, batch 700, training accuracy 1\n",
      "step 15, batch 100, training accuracy 1\n",
      "step 15, batch 200, training accuracy 1\n",
      "step 15, batch 300, training accuracy 0.98\n",
      "step 15, batch 400, training accuracy 1\n",
      "step 15, batch 500, training accuracy 1\n",
      "step 15, batch 600, training accuracy 0.98\n",
      "step 15, batch 700, training accuracy 1\n",
      "step 16, batch 100, training accuracy 1\n",
      "step 16, batch 200, training accuracy 1\n",
      "step 16, batch 300, training accuracy 1\n",
      "step 16, batch 400, training accuracy 1\n",
      "step 16, batch 500, training accuracy 1\n",
      "step 16, batch 600, training accuracy 1\n",
      "step 16, batch 700, training accuracy 1\n",
      "step 17, batch 100, training accuracy 1\n",
      "step 17, batch 200, training accuracy 1\n",
      "step 17, batch 300, training accuracy 1\n",
      "step 17, batch 400, training accuracy 1\n",
      "step 17, batch 500, training accuracy 1\n",
      "step 17, batch 600, training accuracy 1\n",
      "step 17, batch 700, training accuracy 1\n",
      "step 18, batch 100, training accuracy 1\n",
      "step 18, batch 200, training accuracy 1\n",
      "step 18, batch 300, training accuracy 1\n",
      "step 18, batch 400, training accuracy 1\n",
      "step 18, batch 500, training accuracy 1\n",
      "step 18, batch 600, training accuracy 1\n",
      "step 18, batch 700, training accuracy 1\n",
      "step 19, batch 100, training accuracy 1\n",
      "step 19, batch 200, training accuracy 1\n",
      "step 19, batch 300, training accuracy 1\n",
      "step 19, batch 400, training accuracy 1\n",
      "step 19, batch 500, training accuracy 1\n",
      "step 19, batch 600, training accuracy 1\n",
      "step 19, batch 700, training accuracy 1\n",
      "step 20, batch 100, training accuracy 1\n",
      "step 20, batch 200, training accuracy 1\n",
      "step 20, batch 300, training accuracy 1\n",
      "step 20, batch 400, training accuracy 1\n",
      "step 20, batch 500, training accuracy 1\n",
      "step 20, batch 600, training accuracy 1\n",
      "step 20, batch 700, training accuracy 1\n",
      "step 21, batch 100, training accuracy 1\n",
      "step 21, batch 200, training accuracy 1\n",
      "step 21, batch 300, training accuracy 1\n",
      "step 21, batch 400, training accuracy 1\n",
      "step 21, batch 500, training accuracy 1\n",
      "step 21, batch 600, training accuracy 1\n",
      "step 21, batch 700, training accuracy 1\n",
      "step 22, batch 100, training accuracy 1\n",
      "step 22, batch 200, training accuracy 1\n",
      "step 22, batch 300, training accuracy 1\n",
      "step 22, batch 400, training accuracy 1\n",
      "step 22, batch 500, training accuracy 1\n",
      "step 22, batch 600, training accuracy 1\n",
      "step 22, batch 700, training accuracy 1\n",
      "step 23, batch 100, training accuracy 1\n",
      "step 23, batch 200, training accuracy 1\n",
      "step 23, batch 300, training accuracy 1\n",
      "step 23, batch 400, training accuracy 1\n",
      "step 23, batch 500, training accuracy 1\n",
      "step 23, batch 600, training accuracy 1\n",
      "step 23, batch 700, training accuracy 1\n",
      "step 24, batch 100, training accuracy 1\n",
      "step 24, batch 200, training accuracy 1\n",
      "step 24, batch 300, training accuracy 1\n",
      "step 24, batch 400, training accuracy 1\n",
      "step 24, batch 500, training accuracy 1\n",
      "step 24, batch 600, training accuracy 1\n",
      "step 24, batch 700, training accuracy 1\n",
      "test accuracy 0.99\n"
     ]
    }
   ],
   "source": [
    "predictions = run_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                         \"Label\": predictions})\n",
    "submissions.to_csv(\"tf3.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
